{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/luojie1024/TextClassification/blob/main/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T01:36:21.099518Z",
     "start_time": "2023-12-19T01:35:51.593446Z"
    },
    "id": "nsVty8_ooSPQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pypi.douban.com/simple/\n",
      "Requirement already satisfied: jieba in /Users/yzb/anaconda3/envs/model/lib/python3.11/site-packages (0.42.1)\n",
      "Looking in indexes: http://pypi.douban.com/simple/\n",
      "Requirement already satisfied: gensim in /Users/yzb/anaconda3/envs/model/lib/python3.11/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/yzb/anaconda3/envs/model/lib/python3.11/site-packages (from gensim) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/yzb/anaconda3/envs/model/lib/python3.11/site-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/yzb/anaconda3/envs/model/lib/python3.11/site-packages (from gensim) (6.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba\n",
    "!pip install gensim\n",
    "import pandas as pd\n",
    "import json\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91lm3yrJoSPW"
   },
   "source": [
    "# 0. gensim实践：\n",
    "\n",
    "1. 读取预处理好的数据\n",
    "2. 训练\n",
    "3. 完事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xohX5p5PqA4S"
   },
   "source": [
    "# 1. 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3F_6dc-qHFW",
    "outputId": "22c3b4fc-f252-4f25-8494-217d673a3571"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-19 09:40:53--  https://storage.googleapis.com/cluebenchmark/tasks/tnews_public.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.66.91, 142.250.66.155, 172.217.31.27, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.66.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5123575 (4.9M) [application/zip]\n",
      "Saving to: ‘tnews_public.zip’\n",
      "\n",
      "tnews_public.zip    100%[===================>]   4.89M  9.95MB/s    in 0.5s    \n",
      "\n",
      "2023-12-19 09:40:54 (9.95 MB/s) - ‘tnews_public.zip’ saved [5123575/5123575]\n",
      "\n",
      "Archive:  tnews_public.zip\n",
      "  inflating: dev.json                \n",
      "  inflating: labels.json             \n",
      "  inflating: test.json               \n",
      "  inflating: test1.0.json            \n",
      "  inflating: train.json              \n",
      "  inflating: README.txt              \n"
     ]
    }
   ],
   "source": [
    "! wget https://storage.googleapis.com/cluebenchmark/tasks/tnews_public.zip \n",
    "! unzip tnews_public.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkWzdXyKqTIE"
   },
   "source": [
    "## 1.1 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wFn6Gwu9smgq"
   },
   "outputs": [],
   "source": [
    "def get_sentence(data_file):\n",
    "    # 读取数据集中的句子\n",
    "    f=open('train.json','r',encoding='utf-8')\n",
    "    reader = f.readlines()\n",
    "    sentence=[]\n",
    "    for line in reader:\n",
    "        line=json.loads(line.strip())\n",
    "        sentence.append(line['sentence'])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "crZAcL_zs-fW"
   },
   "outputs": [],
   "source": [
    "# 读取句子语料\n",
    "train_sentence=get_sentence('train.json')\n",
    "test_sentence=get_sentence('test.json')\n",
    "dev_sentence=get_sentence('dev.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtvnPU2toSPX"
   },
   "source": [
    "# 2. 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dakJhzKPtr3L"
   },
   "outputs": [],
   "source": [
    "# 使用所有语料作为词向量训练语料\n",
    "train_data=train_sentence+test_sentence+dev_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gS1xii-dt3iD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/g0/wqv_f7ds0m91wfk_jx58wsrc0000gn/T/jieba.cache\n",
      "Loading model cost 0.306 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.55 s, sys: 47.4 ms, total: 4.6 s\n",
      "Wall time: 4.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 分词处理\n",
    "train_data=[list(jieba.cut(stentence)) for stentence in train_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBqwvzdyoSPY"
   },
   "source": [
    "# 3. 模型创建\n",
    "\n",
    "Gensim中 Word2Vec 模型的期望输入是进过分词的句子列表，即是某个二维数组。这里我们暂时使用 Python 内置的数组，不过其在输入数据集较大的情况下会占用大量的 RAM。Gensim 本身只是要求能够迭代的有序句子列表，因此在工程实践中我们可以使用自定义的生成器，只在内存中保存单条语句。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ut8oHJ33oSPY"
   },
   "source": [
    "## Word2Vec 参数\n",
    "+ min_count\n",
    "\n",
    "在不同大小的语料集中，我们对于基准词频的需求也是不一样的。譬如在较大的语料集中，我们希望忽略那些只出现过一两次的单词，这里我们就可以通过设置min_count参数进行控制。一般而言，合理的参数值会设置在0~100之间。\n",
    "\n",
    "+ size\n",
    "\n",
    "size参数主要是用来设置神经网络的层数，Word2Vec 中的默认值是设置为100层。更大的层次设置意味着更多的输入数据，不过也能提升整体的准确度，合理的设置范围为 10~数百。\n",
    "\n",
    "+ workers\n",
    "\n",
    "workers参数用于设置并发训练时候的线程数，不过仅当Cython安装的情况下才会起作用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "u5N9mutooSPY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 引入 word2vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "\n",
    "# 引入日志配置\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLYeikE5oSPZ"
   },
   "source": [
    "# 构建训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAWmSnGax3Xc"
   },
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "dqbCqrCqxxGW"
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FastText in module gensim.models.fasttext:\n",
      "\n",
      "class FastText(gensim.models.word2vec.Word2Vec)\n",
      " |  FastText(sentences=None, corpus_file=None, sg=0, hs=0, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, word_ngrams=1, sample=0.001, seed=1, workers=3, min_alpha=0.0001, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, min_n=3, max_n=6, sorted_vocab=1, bucket=2000000, trim_rule=None, batch_words=10000, callbacks=(), max_final_vocab=None, shrink_windows=True)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      FastText\n",
      " |      gensim.models.word2vec.Word2Vec\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sentences=None, corpus_file=None, sg=0, hs=0, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, word_ngrams=1, sample=0.001, seed=1, workers=3, min_alpha=0.0001, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, min_n=3, max_n=6, sorted_vocab=1, bucket=2000000, trim_rule=None, batch_words=10000, callbacks=(), max_final_vocab=None, shrink_windows=True)\n",
      " |      Train, use and evaluate word representations learned using the method\n",
      " |      described in `Enriching Word Vectors with Subword Information <https://arxiv.org/abs/1607.04606>`_,\n",
      " |      aka FastText.\n",
      " |      \n",
      " |      The model can be stored/loaded via its :meth:`~gensim.models.fasttext.FastText.save` and\n",
      " |      :meth:`~gensim.models.fasttext.FastText.load` methods, or loaded from a format compatible with the\n",
      " |      original Fasttext implementation via :func:`~gensim.models.fasttext.load_facebook_model`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str, optional\n",
      " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus'\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such\n",
      " |          examples. If you don't supply `sentences`, the model is left uninitialized -- use if you plan to\n",
      " |          initialize it in some other way.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left\n",
      " |          uninitialized).\n",
      " |      min_count : int, optional\n",
      " |          The model ignores all words with total frequency lower than this.\n",
      " |      vector_size : int, optional\n",
      " |          Dimensionality of the word vectors.\n",
      " |      window : int, optional\n",
      " |          The maximum distance between the current and predicted word within a sentence.\n",
      " |      workers : int, optional\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      sg : {1, 0}, optional\n",
      " |          Training algorithm: skip-gram if `sg=1`, otherwise CBOW.\n",
      " |      hs : {1,0}, optional\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If set to 0, and `negative` is non-zero, negative sampling will be used.\n",
      " |      seed : int, optional\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      " |      max_vocab_size : int, optional\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      sample : float, optional\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      negative : int, optional\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If set to 0, no negative sampling is used.\n",
      " |      ns_exponent : float, optional\n",
      " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      " |          other values may perform better for recommendation applications.\n",
      " |      cbow_mean : {1,0}, optional\n",
      " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      " |      hashfxn : function, optional\n",
      " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      " |      iter : int, optional\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during\n",
      " |          :meth:`~gensim.models.fasttext.FastText.build_vocab` and is not stored as part of themodel.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      sorted_vocab : {1,0}, optional\n",
      " |          If 1, sort the vocabulary by descending frequency before assigning word indices.\n",
      " |      batch_words : int, optional\n",
      " |          Target size (in words) for batches of examples passed to worker threads (and\n",
      " |          thus cython routines).(Larger batches will be passed if individual\n",
      " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      " |      min_n : int, optional\n",
      " |          Minimum length of char n-grams to be used for training word representations.\n",
      " |      max_n : int, optional\n",
      " |          Max length of char ngrams to be used for training word representations. Set `max_n` to be\n",
      " |          lesser than `min_n` to avoid char ngrams being used.\n",
      " |      word_ngrams : int, optional\n",
      " |          In Facebook's FastText, \"max length of word ngram\" - but gensim only supports the\n",
      " |          default of 1 (regular unigram word handling).\n",
      " |      bucket : int, optional\n",
      " |          Character ngrams are hashed into a fixed number of buckets, in order to limit the\n",
      " |          memory usage of the model. This option specifies the number of buckets used by the model.\n",
      " |          The default value of 2000000 consumes as much memory as having 2000000 more in-vocabulary\n",
      " |          words in your model.\n",
      " |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          List of callbacks that need to be executed/run at specific stages during training.\n",
      " |      max_final_vocab : int, optional\n",
      " |          Limits the vocab to a target vocab size by automatically selecting\n",
      " |          ``min_count```.  If the specified ``min_count`` is more than the\n",
      " |          automatically calculated ``min_count``, the former will be used.\n",
      " |          Set to ``None`` if not required.\n",
      " |      shrink_windows : bool, optional\n",
      " |          New in 4.1. Experimental.\n",
      " |          If True, the effective window size is uniformly sampled from  [1, `window`]\n",
      " |          for each target word during training, to match the original word2vec algorithm's\n",
      " |          approximate weighting of context words by distance. Otherwise, the effective\n",
      " |          window size is always fixed to `window` words to either side.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Initialize and train a `FastText` model:\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import FastText\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>>\n",
      " |          >>> model = FastText(sentences, min_count=1)\n",
      " |          >>> say_vector = model.wv['say']  # get vector for word\n",
      " |          >>> of_vector = model.wv['of']  # get vector for out-of-vocab word\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      wv : :class:`~gensim.models.fasttext.FastTextKeyedVectors`\n",
      " |          This object essentially contains the mapping between words and embeddings. These are similar to\n",
      " |          the embedding computed in the :class:`~gensim.models.word2vec.Word2Vec`, however here we also\n",
      " |          include vectors for n-grams. This allows the model to compute embeddings even for **unseen**\n",
      " |          words (that do not exist in the vocabulary), as the aggregate of the n-grams included in the word.\n",
      " |          After training the model, this attribute can be used directly to query those embeddings in various\n",
      " |          ways. Check the module level docstring for some examples.\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate memory that will be needed to train a model, and print the estimates to log.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors. Obsoleted.\n",
      " |      \n",
      " |      If you need a single unit-normalized vector for some key, call\n",
      " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.get_vector` instead:\n",
      " |      ``fasttext_model.wv.get_vector(key, norm=True)``.\n",
      " |      \n",
      " |      To refresh norms after you performed some atypical out-of-band vector tampering,\n",
      " |      call `:meth:`~gensim.models.keyedvectors.KeyedVectors.fill_norms()` instead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool\n",
      " |          If True, forget the original trained vectors and only keep the normalized ones.\n",
      " |          You lose information if you do this.\n",
      " |  \n",
      " |  load_binary_data(self, encoding='utf8')\n",
      " |      Load data from a binary file created by Facebook's native FastText.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      encoding : str, optional\n",
      " |          Specifies the encoding.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the Fasttext model. This saved model can be loaded again using\n",
      " |      :meth:`~gensim.models.fasttext.FastText.load`, which supports incremental training\n",
      " |      and getting vectors for out-of-vocabulary words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Store the model to this file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastText.load`\n",
      " |          Load :class:`~gensim.models.fasttext.FastText` model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved `FastText` model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.fasttext.FastText`\n",
      " |          Loaded model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastText.save`\n",
      " |          Save :class:`~gensim.models.fasttext.FastText` model.\n",
      " |  \n",
      " |  load_fasttext_format(model_file, encoding='utf8') from builtins.type\n",
      " |      Deprecated.\n",
      " |      \n",
      " |      Use :func:`gensim.models.fasttext.load_facebook_model` or\n",
      " |      :func:`gensim.models.fasttext.load_facebook_vectors` instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.word2vec.Word2Vec:\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Human readable representation of the model's state.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
      " |          and learning rate.\n",
      " |  \n",
      " |  add_null_word(self)\n",
      " |  \n",
      " |  build_vocab(self, corpus_iterable=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus_iterable : iterable of list of str\n",
      " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      update : bool\n",
      " |          If true, the new words in `sentences` will be added to model's vocab.\n",
      " |      progress_per : int, optional\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      **kwargs : object\n",
      " |          Keyword arguments propagated to `self.prepare_vocab`.\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict of (str, int)\n",
      " |          A mapping from a word in the vocabulary to its frequency count.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      " |      corpus_count : int, optional\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      update : bool, optional\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |  \n",
      " |  create_binary_tree(self)\n",
      " |      Create a `binary Huffman tree <https://en.wikipedia.org/wiki/Huffman_coding>`_ using stored vocabulary\n",
      " |      word counts. Frequent words will have shorter binary codes.\n",
      " |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
      " |  \n",
      " |  get_latest_training_loss(self)\n",
      " |      Get current value of the training loss.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Current training loss.\n",
      " |  \n",
      " |  init_weights(self)\n",
      " |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n",
      " |  \n",
      " |  make_cum_table(self, domain=2147483647)\n",
      " |      Create a cumulative-distribution table using stored vocabulary word counts for\n",
      " |      drawing random words in the negative-sampling training routines.\n",
      " |      \n",
      " |      To draw a word index, choose a random integer up to the maximum value in the table (cum_table[-1]),\n",
      " |      then finding that integer's sorted insertion point (as if by `bisect_left` or `ndarray.searchsorted()`).\n",
      " |      That insertion point is the drawn index, coming up in proportion equal to the increment at that slot.\n",
      " |  \n",
      " |  predict_output_word(self, context_words_list, topn=10)\n",
      " |      Get the probability distribution of the center word given context words.\n",
      " |      \n",
      " |      Note this performs a CBOW-style propagation, even in SG models,\n",
      " |      and doesn't quite weight the surrounding words the same as in\n",
      " |      training -- so it's just one crude way of using a trained model\n",
      " |      as a predictor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      context_words_list : list of (str and/or int)\n",
      " |          List of context words, which may be words themselves (str)\n",
      " |          or their index in `self.wv.vectors` (int).\n",
      " |      topn : int, optional\n",
      " |          Return `topn` words and their probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          `topn` length list of tuples of (word, probability).\n",
      " |  \n",
      " |  prepare_vocab(self, update=False, keep_raw_vocab=False, trim_rule=None, min_count=None, sample=None, dry_run=False)\n",
      " |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
      " |      and `sample` (controlling the downsampling of more-frequent words).\n",
      " |      \n",
      " |      Calling with `dry_run=True` will only simulate the provided settings and\n",
      " |      report the size of the retained vocabulary, effective corpus length, and\n",
      " |      estimated memory requirements. Results are both printed via logging and\n",
      " |      returned as a dict.\n",
      " |      \n",
      " |      Delete the raw vocabulary after the scaling is done to free up RAM,\n",
      " |      unless `keep_raw_vocab` is set.\n",
      " |  \n",
      " |  prepare_weights(self, update=False)\n",
      " |      Build tables and model weights based on final vocabulary settings.\n",
      " |  \n",
      " |  reset_from(self, other_model)\n",
      " |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
      " |      \n",
      " |      Structures copied are:\n",
      " |          * Vocabulary\n",
      " |          * Index to word mapping\n",
      " |          * Cumulative frequency table (used for negative sampling)\n",
      " |          * Cached corpus length\n",
      " |      \n",
      " |      Useful when testing multiple models on the same corpus in parallel. However, as the models\n",
      " |      then share all vocabulary-related structures other than vectors, neither should then\n",
      " |      expand their vocabulary (which could leave the other in an inconsistent, broken state).\n",
      " |      And, any changes to any per-word 'vecattr' will affect both models.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Another model to copy the internal structures from.\n",
      " |  \n",
      " |  scan_vocab(self, corpus_iterable=None, corpus_file=None, progress_per=10000, workers=None, trim_rule=None)\n",
      " |  \n",
      " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      " |      Score the log probability for a sequence of sentences.\n",
      " |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      " |      \n",
      " |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      " |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      " |      \n",
      " |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      " |      \n",
      " |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      " |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      " |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      " |      how to use such scores in document classification.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |      total_sentences : int, optional\n",
      " |          Count of sentences.\n",
      " |      chunksize : int, optional\n",
      " |          Chunksize of jobs\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |  \n",
      " |  seeded_vector(self, seed_string, vector_size)\n",
      " |  \n",
      " |  train(self, corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=(), **kwargs)\n",
      " |      Update the model's neural weights from a sequence of sentences.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      " |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      " |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      " |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      " |      you can simply use `total_examples=self.corpus_count`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      " |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.epochs`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus_iterable : iterable of list of str\n",
      " |          The ``corpus_iterable`` can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network, to limit RAM usage.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      total_examples : int\n",
      " |          Count of sentences.\n",
      " |      total_words : int\n",
      " |          Count of raw words in sentences.\n",
      " |      epochs : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float, optional\n",
      " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      " |          for this one call to`train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      end_alpha : float, optional\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      word_count : int, optional\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in sentences.\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>>\n",
      " |          >>> model = Word2Vec(min_count=1)\n",
      " |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      " |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors\n",
      " |          (1, 30)\n",
      " |  \n",
      " |  update_weights(self)\n",
      " |      Copy all the existing weights, and reset the weights for the newly added vocabulary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |      \n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |      \n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |      \n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |      \n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |      \n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H2mZIENAx9Sw",
    "outputId": "6db9790a-8103-4ece-9771-4943980c0bf2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 09:47:17,498 : INFO : collecting all words and their counts\n",
      "2023-12-19 09:47:17,500 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-12-19 09:47:17,529 : INFO : PROGRESS: at sentence #10000, processed 129141 words, keeping 25698 word types\n",
      "2023-12-19 09:47:17,549 : INFO : PROGRESS: at sentence #20000, processed 258779 words, keeping 39123 word types\n",
      "2023-12-19 09:47:17,566 : INFO : PROGRESS: at sentence #30000, processed 387255 words, keeping 49324 word types\n",
      "2023-12-19 09:47:17,580 : INFO : PROGRESS: at sentence #40000, processed 515598 words, keeping 57864 word types\n",
      "2023-12-19 09:47:17,593 : INFO : PROGRESS: at sentence #50000, processed 644895 words, keeping 65382 word types\n",
      "2023-12-19 09:47:17,605 : INFO : PROGRESS: at sentence #60000, processed 774201 words, keeping 67815 word types\n",
      "2023-12-19 09:47:17,617 : INFO : PROGRESS: at sentence #70000, processed 903891 words, keeping 67815 word types\n",
      "2023-12-19 09:47:17,629 : INFO : PROGRESS: at sentence #80000, processed 1032613 words, keeping 67815 word types\n",
      "2023-12-19 09:47:17,640 : INFO : PROGRESS: at sentence #90000, processed 1161529 words, keeping 67815 word types\n",
      "2023-12-19 09:47:17,653 : INFO : PROGRESS: at sentence #100000, processed 1289998 words, keeping 67815 word types\n",
      "2023-12-19 09:47:17,665 : INFO : PROGRESS: at sentence #110000, processed 1419372 words, keeping 67815 word types\n",
      "2023-12-19 09:47:17,675 : INFO : PROGRESS: at sentence #120000, processed 1548930 words, keeping 67815 word types\n",
      "2023-12-19 09:47:17,687 : INFO : PROGRESS: at sentence #130000, processed 1678029 words, keeping 67815 word types\n",
      "2023-12-19 09:47:17,699 : INFO : PROGRESS: at sentence #140000, processed 1807295 words, keeping 67815 word types\n",
      "2023-12-19 09:47:17,711 : INFO : PROGRESS: at sentence #150000, processed 1935386 words, keeping 67815 word types\n",
      "2023-12-19 09:47:17,724 : INFO : PROGRESS: at sentence #160000, processed 2064737 words, keeping 67815 word types\n",
      "2023-12-19 09:47:17,724 : INFO : collected 67815 word types from a corpus of 2065815 raw words and 160080 sentences\n",
      "2023-12-19 09:47:17,724 : INFO : Creating a fresh vocabulary\n",
      "2023-12-19 09:47:17,789 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 retains 67815 unique words (100.00% of original 67815, drops 0)', 'datetime': '2023-12-19T09:47:17.789791', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-12-19 09:47:17,790 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 leaves 2065815 word corpus (100.00% of original 2065815, drops 0)', 'datetime': '2023-12-19T09:47:17.790152', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-12-19 09:47:17,888 : INFO : deleting the raw counts dictionary of 67815 items\n",
      "2023-12-19 09:47:17,889 : INFO : sample=0.001 downsamples 27 most-common words\n",
      "2023-12-19 09:47:17,890 : INFO : FastText lifecycle event {'msg': 'downsampling leaves estimated 1743150.9124933355 word corpus (84.4%% of prior 2065815)', 'datetime': '2023-12-19T09:47:17.890013', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-12-19 09:47:18,120 : INFO : estimated required memory for 67815 words, 2000000 buckets and 100 dimensions: 896219608 bytes\n",
      "2023-12-19 09:47:18,121 : INFO : resetting layer weights\n",
      "2023-12-19 09:47:19,042 : INFO : FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-12-19T09:47:19.042014', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2023-12-19 09:47:19,042 : INFO : FastText lifecycle event {'msg': 'training model with 3 workers on 67815 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=3 shrink_windows=True', 'datetime': '2023-12-19T09:47:19.042485', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-arm64-arm-64bit', 'event': 'train'}\n",
      "2023-12-19 09:47:20,052 : INFO : EPOCH 0 - PROGRESS: at 87.54% examples, 1514907 words/s, in_qsize 5, out_qsize 0\n",
      "2023-12-19 09:47:20,185 : INFO : EPOCH 0: training on 2065815 raw words (1743284 effective words) took 1.1s, 1529203 effective words/s\n",
      "2023-12-19 09:47:21,197 : INFO : EPOCH 1 - PROGRESS: at 89.00% examples, 1535580 words/s, in_qsize 5, out_qsize 0\n",
      "2023-12-19 09:47:21,313 : INFO : EPOCH 1: training on 2065815 raw words (1743349 effective words) took 1.1s, 1547996 effective words/s\n",
      "2023-12-19 09:47:22,326 : INFO : EPOCH 2 - PROGRESS: at 89.00% examples, 1536282 words/s, in_qsize 5, out_qsize 0\n",
      "2023-12-19 09:47:22,442 : INFO : EPOCH 2: training on 2065815 raw words (1742863 effective words) took 1.1s, 1548803 effective words/s\n",
      "2023-12-19 09:47:23,458 : INFO : EPOCH 3 - PROGRESS: at 89.00% examples, 1532497 words/s, in_qsize 6, out_qsize 0\n",
      "2023-12-19 09:47:23,575 : INFO : EPOCH 3: training on 2065815 raw words (1742835 effective words) took 1.1s, 1543905 effective words/s\n",
      "2023-12-19 09:47:24,578 : INFO : EPOCH 4 - PROGRESS: at 87.07% examples, 1515681 words/s, in_qsize 5, out_qsize 0\n",
      "2023-12-19 09:47:24,724 : INFO : EPOCH 4: training on 2065815 raw words (1743367 effective words) took 1.1s, 1519385 effective words/s\n",
      "2023-12-19 09:47:24,725 : INFO : FastText lifecycle event {'msg': 'training on 10329075 raw words (8715698 effective words) took 5.7s, 1533797 effective words/s', 'datetime': '2023-12-19T09:47:24.725033', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-arm64-arm-64bit', 'event': 'train'}\n",
      "2023-12-19 09:47:24,991 : INFO : FastText lifecycle event {'params': 'FastText<vocab=67815, vector_size=100, alpha=0.025>', 'datetime': '2023-12-19T09:47:24.991246', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "model = FastText(train_data,  window=3, min_count=1,min_n = 3 , max_n = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvcIZfYaxzc3"
   },
   "source": [
    "## skip-gram 与 CBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k3IW_UukoSPZ",
    "outputId": "fdc2328c-e9d7-44af-d828-8a622859fb18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 09:49:29,259 : INFO : collecting all words and their counts\n",
      "2023-12-19 09:49:29,260 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-12-19 09:49:29,292 : INFO : PROGRESS: at sentence #10000, processed 129141 words, keeping 25698 word types\n",
      "2023-12-19 09:49:29,311 : INFO : PROGRESS: at sentence #20000, processed 258779 words, keeping 39123 word types\n",
      "2023-12-19 09:49:29,327 : INFO : PROGRESS: at sentence #30000, processed 387255 words, keeping 49324 word types\n",
      "2023-12-19 09:49:29,339 : INFO : PROGRESS: at sentence #40000, processed 515598 words, keeping 57864 word types\n",
      "2023-12-19 09:49:29,351 : INFO : PROGRESS: at sentence #50000, processed 644895 words, keeping 65382 word types\n",
      "2023-12-19 09:49:29,360 : INFO : PROGRESS: at sentence #60000, processed 774201 words, keeping 67815 word types\n",
      "2023-12-19 09:49:29,369 : INFO : PROGRESS: at sentence #70000, processed 903891 words, keeping 67815 word types\n",
      "2023-12-19 09:49:29,379 : INFO : PROGRESS: at sentence #80000, processed 1032613 words, keeping 67815 word types\n",
      "2023-12-19 09:49:29,389 : INFO : PROGRESS: at sentence #90000, processed 1161529 words, keeping 67815 word types\n",
      "2023-12-19 09:49:29,400 : INFO : PROGRESS: at sentence #100000, processed 1289998 words, keeping 67815 word types\n",
      "2023-12-19 09:49:29,411 : INFO : PROGRESS: at sentence #110000, processed 1419372 words, keeping 67815 word types\n",
      "2023-12-19 09:49:29,420 : INFO : PROGRESS: at sentence #120000, processed 1548930 words, keeping 67815 word types\n",
      "2023-12-19 09:49:29,430 : INFO : PROGRESS: at sentence #130000, processed 1678029 words, keeping 67815 word types\n",
      "2023-12-19 09:49:29,440 : INFO : PROGRESS: at sentence #140000, processed 1807295 words, keeping 67815 word types\n",
      "2023-12-19 09:49:29,451 : INFO : PROGRESS: at sentence #150000, processed 1935386 words, keeping 67815 word types\n",
      "2023-12-19 09:49:29,462 : INFO : PROGRESS: at sentence #160000, processed 2064737 words, keeping 67815 word types\n",
      "2023-12-19 09:49:29,462 : INFO : collected 67815 word types from a corpus of 2065815 raw words and 160080 sentences\n",
      "2023-12-19 09:49:29,463 : INFO : Creating a fresh vocabulary\n",
      "2023-12-19 09:49:29,498 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 31842 unique words (46.95% of original 67815, drops 35973)', 'datetime': '2023-12-19T09:49:29.498226', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-12-19 09:49:29,498 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1957896 word corpus (94.78% of original 2065815, drops 107919)', 'datetime': '2023-12-19T09:49:29.498586', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-12-19 09:49:29,545 : INFO : deleting the raw counts dictionary of 67815 items\n",
      "2023-12-19 09:49:29,546 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2023-12-19 09:49:29,547 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1627809.560462399 word corpus (83.1%% of prior 1957896)', 'datetime': '2023-12-19T09:49:29.547049', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-12-19 09:49:29,627 : INFO : estimated required memory for 31842 words and 100 dimensions: 41394600 bytes\n",
      "2023-12-19 09:49:29,627 : INFO : resetting layer weights\n",
      "2023-12-19 09:49:29,636 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-12-19T09:49:29.636438', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2023-12-19 09:49:29,636 : INFO : Word2Vec lifecycle event {'msg': 'training model with 8 workers on 31842 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-12-19T09:49:29.636678', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-arm64-arm-64bit', 'event': 'train'}\n",
      "2023-12-19 09:49:30,642 : INFO : EPOCH 0 - PROGRESS: at 88.51% examples, 1438184 words/s, in_qsize 15, out_qsize 0\n",
      "2023-12-19 09:49:30,763 : INFO : EPOCH 0: training on 2065815 raw words (1627473 effective words) took 1.1s, 1449252 effective words/s\n",
      "2023-12-19 09:49:31,769 : INFO : EPOCH 1 - PROGRESS: at 91.47% examples, 1485444 words/s, in_qsize 16, out_qsize 0\n",
      "2023-12-19 09:49:31,846 : INFO : EPOCH 1: training on 2065815 raw words (1627393 effective words) took 1.1s, 1508076 effective words/s\n",
      "2023-12-19 09:49:32,860 : INFO : EPOCH 2 - PROGRESS: at 89.49% examples, 1442021 words/s, in_qsize 14, out_qsize 1\n",
      "2023-12-19 09:49:32,958 : INFO : EPOCH 2: training on 2065815 raw words (1627472 effective words) took 1.1s, 1468252 effective words/s\n",
      "2023-12-19 09:49:33,963 : INFO : EPOCH 3 - PROGRESS: at 89.50% examples, 1455377 words/s, in_qsize 14, out_qsize 1\n",
      "2023-12-19 09:49:34,054 : INFO : EPOCH 3: training on 2065815 raw words (1627982 effective words) took 1.1s, 1491086 effective words/s\n",
      "2023-12-19 09:49:35,060 : INFO : EPOCH 4 - PROGRESS: at 90.49% examples, 1469950 words/s, in_qsize 15, out_qsize 0\n",
      "2023-12-19 09:49:35,151 : INFO : EPOCH 4: training on 2065815 raw words (1627778 effective words) took 1.1s, 1488805 effective words/s\n",
      "2023-12-19 09:49:35,151 : INFO : Word2Vec lifecycle event {'msg': 'training on 10329075 raw words (8138098 effective words) took 5.5s, 1475616 effective words/s', 'datetime': '2023-12-19T09:49:35.151809', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-arm64-arm-64bit', 'event': 'train'}\n",
      "2023-12-19 09:49:35,152 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=31842, vector_size=100, alpha=0.025>', 'datetime': '2023-12-19T09:49:35.152022', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# sg : {0, 1}, optional Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "model = word2vec.Word2Vec(train_data, sg=1,workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module gensim.models.word2vec in gensim.models:\n",
      "\n",
      "NAME\n",
      "    gensim.models.word2vec\n",
      "\n",
      "DESCRIPTION\n",
      "    Introduction\n",
      "    ============\n",
      "    \n",
      "    This module implements the word2vec family of algorithms, using highly optimized C routines,\n",
      "    data streaming and Pythonic interfaces.\n",
      "    \n",
      "    The word2vec algorithms include skip-gram and CBOW models, using either\n",
      "    hierarchical softmax or negative sampling: `Tomas Mikolov et al: Efficient Estimation of Word Representations\n",
      "    in Vector Space <https://arxiv.org/pdf/1301.3781.pdf>`_, `Tomas Mikolov et al: Distributed Representations of Words\n",
      "    and Phrases and their Compositionality <https://arxiv.org/abs/1310.4546>`_.\n",
      "    \n",
      "    Other embeddings\n",
      "    ================\n",
      "    \n",
      "    There are more ways to train word vectors in Gensim than just Word2Vec.\n",
      "    See also :class:`~gensim.models.doc2vec.Doc2Vec`, :class:`~gensim.models.fasttext.FastText`.\n",
      "    \n",
      "    The training algorithms were originally ported from the C package https://code.google.com/p/word2vec/\n",
      "    and extended with additional functionality and\n",
      "    `optimizations <https://rare-technologies.com/parallelizing-word2vec-in-python/>`_ over the years.\n",
      "    \n",
      "    For a tutorial on Gensim word2vec, with an interactive web app trained on GoogleNews,\n",
      "    visit https://rare-technologies.com/word2vec-tutorial/.\n",
      "    \n",
      "    Usage examples\n",
      "    ==============\n",
      "    \n",
      "    Initialize a model with e.g.:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.test.utils import common_texts\n",
      "        >>> from gensim.models import Word2Vec\n",
      "        >>>\n",
      "        >>> model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
      "        >>> model.save(\"word2vec.model\")\n",
      "    \n",
      "    \n",
      "    **The training is streamed, so ``sentences`` can be an iterable**, reading input data\n",
      "    from the disk or network on-the-fly, without loading your entire corpus into RAM.\n",
      "    \n",
      "    Note the ``sentences`` iterable must be *restartable* (not just a generator), to allow the algorithm\n",
      "    to stream over your dataset multiple times. For some examples of streamed iterables,\n",
      "    see :class:`~gensim.models.word2vec.BrownCorpus`,\n",
      "    :class:`~gensim.models.word2vec.Text8Corpus` or :class:`~gensim.models.word2vec.LineSentence`.\n",
      "    \n",
      "    If you save the model you can continue training it later:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> model = Word2Vec.load(\"word2vec.model\")\n",
      "        >>> model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)\n",
      "        (0, 2)\n",
      "    \n",
      "    The trained word vectors are stored in a :class:`~gensim.models.keyedvectors.KeyedVectors` instance, as `model.wv`:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> vector = model.wv['computer']  # get numpy vector of a word\n",
      "        >>> sims = model.wv.most_similar('computer', topn=10)  # get other similar words\n",
      "    \n",
      "    The reason for separating the trained vectors into `KeyedVectors` is that if you don't\n",
      "    need the full model state any more (don't need to continue training), its state can be discarded,\n",
      "    keeping just the vectors and their keys proper.\n",
      "    \n",
      "    This results in a much smaller and faster object that can be mmapped for lightning\n",
      "    fast loading and sharing the vectors in RAM between processes:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.models import KeyedVectors\n",
      "        >>>\n",
      "        >>> # Store just the words + their trained embeddings.\n",
      "        >>> word_vectors = model.wv\n",
      "        >>> word_vectors.save(\"word2vec.wordvectors\")\n",
      "        >>>\n",
      "        >>> # Load back with memory-mapping = read-only, shared across processes.\n",
      "        >>> wv = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')\n",
      "        >>>\n",
      "        >>> vector = wv['computer']  # Get numpy vector of a word\n",
      "    \n",
      "    Gensim can also load word vectors in the \"word2vec C format\", as a\n",
      "    :class:`~gensim.models.keyedvectors.KeyedVectors` instance:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.test.utils import datapath\n",
      "        >>>\n",
      "        >>> # Load a word2vec model stored in the C *text* format.\n",
      "        >>> wv_from_text = KeyedVectors.load_word2vec_format(datapath('word2vec_pre_kv_c'), binary=False)\n",
      "        >>> # Load a word2vec model stored in the C *binary* format.\n",
      "        >>> wv_from_bin = KeyedVectors.load_word2vec_format(datapath(\"euclidean_vectors.bin\"), binary=True)\n",
      "    \n",
      "    It is impossible to continue training the vectors loaded from the C format because the hidden weights,\n",
      "    vocabulary frequencies and the binary tree are missing. To continue training, you'll need the\n",
      "    full :class:`~gensim.models.word2vec.Word2Vec` object state, as stored by :meth:`~gensim.models.word2vec.Word2Vec.save`,\n",
      "    not just the :class:`~gensim.models.keyedvectors.KeyedVectors`.\n",
      "    \n",
      "    You can perform various NLP tasks with a trained model. Some of the operations\n",
      "    are already built-in - see :mod:`gensim.models.keyedvectors`.\n",
      "    \n",
      "    If you're finished training a model (i.e. no more updates, only querying),\n",
      "    you can switch to the :class:`~gensim.models.keyedvectors.KeyedVectors` instance:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> word_vectors = model.wv\n",
      "        >>> del model\n",
      "    \n",
      "    to trim unneeded model state = use much less RAM and allow fast loading and memory sharing (mmap).\n",
      "    \n",
      "    Embeddings with multiword ngrams\n",
      "    ================================\n",
      "    \n",
      "    There is a :mod:`gensim.models.phrases` module which lets you automatically\n",
      "    detect phrases longer than one word, using collocation statistics.\n",
      "    Using phrases, you can learn a word2vec model where \"words\" are actually multiword expressions,\n",
      "    such as `new_york_times` or `financial_crisis`:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.models import Phrases\n",
      "        >>>\n",
      "        >>> # Train a bigram detector.\n",
      "        >>> bigram_transformer = Phrases(common_texts)\n",
      "        >>>\n",
      "        >>> # Apply the trained MWE detector to a corpus, using the result to train a Word2vec model.\n",
      "        >>> model = Word2Vec(bigram_transformer[common_texts], min_count=1)\n",
      "    \n",
      "    Pretrained models\n",
      "    =================\n",
      "    \n",
      "    Gensim comes with several already pre-trained models, in the\n",
      "    `Gensim-data repository <https://github.com/RaRe-Technologies/gensim-data>`_:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> import gensim.downloader\n",
      "        >>> # Show all available models in gensim-data\n",
      "        >>> print(list(gensim.downloader.info()['models'].keys()))\n",
      "        ['fasttext-wiki-news-subwords-300',\n",
      "         'conceptnet-numberbatch-17-06-300',\n",
      "         'word2vec-ruscorpora-300',\n",
      "         'word2vec-google-news-300',\n",
      "         'glove-wiki-gigaword-50',\n",
      "         'glove-wiki-gigaword-100',\n",
      "         'glove-wiki-gigaword-200',\n",
      "         'glove-wiki-gigaword-300',\n",
      "         'glove-twitter-25',\n",
      "         'glove-twitter-50',\n",
      "         'glove-twitter-100',\n",
      "         'glove-twitter-200',\n",
      "         '__testing_word2vec-matrix-synopsis']\n",
      "        >>>\n",
      "        >>> # Download the \"glove-twitter-25\" embeddings\n",
      "        >>> glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
      "        >>>\n",
      "        >>> # Use the downloaded vectors as usual:\n",
      "        >>> glove_vectors.most_similar('twitter')\n",
      "        [('facebook', 0.948005199432373),\n",
      "         ('tweet', 0.9403423070907593),\n",
      "         ('fb', 0.9342358708381653),\n",
      "         ('instagram', 0.9104824066162109),\n",
      "         ('chat', 0.8964964747428894),\n",
      "         ('hashtag', 0.8885937333106995),\n",
      "         ('tweets', 0.8878158330917358),\n",
      "         ('tl', 0.8778461217880249),\n",
      "         ('link', 0.8778210878372192),\n",
      "         ('internet', 0.8753897547721863)]\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        BrownCorpus\n",
      "        LineSentence\n",
      "        PathLineSentences\n",
      "        Text8Corpus\n",
      "    Heapitem(builtins.tuple)\n",
      "        Heapitem\n",
      "    gensim.utils.SaveLoad(builtins.object)\n",
      "        Word2Vec\n",
      "        Word2VecTrainables\n",
      "        Word2VecVocab\n",
      "    \n",
      "    class BrownCorpus(builtins.object)\n",
      "     |  BrownCorpus(dirname)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, dirname)\n",
      "     |      Iterate over sentences from the `Brown corpus <https://en.wikipedia.org/wiki/Brown_Corpus>`_\n",
      "     |      (part of `NLTK data <https://www.nltk.org/data.html>`_).\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Heapitem(Heapitem)\n",
      "     |  Heapitem(count, index, left, right)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Heapitem\n",
      "     |      Heapitem\n",
      "     |      builtins.tuple\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __lt__(self, other)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Heapitem:\n",
      "     |  \n",
      "     |  __getnewargs__(self)\n",
      "     |      Return self as a plain tuple.  Used by copy and pickle.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return a nicely formatted representation string\n",
      "     |  \n",
      "     |  _asdict(self)\n",
      "     |      Return a new dict which maps field names to their values.\n",
      "     |  \n",
      "     |  _replace(self, /, **kwds)\n",
      "     |      Return a new Heapitem object replacing specified fields with new values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Heapitem:\n",
      "     |  \n",
      "     |  _make(iterable) from builtins.type\n",
      "     |      Make a new Heapitem object from a sequence or iterable\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from Heapitem:\n",
      "     |  \n",
      "     |  __new__(_cls, count, index, left, right)\n",
      "     |      Create new instance of Heapitem(count, index, left, right)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Heapitem:\n",
      "     |  \n",
      "     |  count\n",
      "     |      Alias for field number 0\n",
      "     |  \n",
      "     |  index\n",
      "     |      Alias for field number 1\n",
      "     |  \n",
      "     |  left\n",
      "     |      Alias for field number 2\n",
      "     |  \n",
      "     |  right\n",
      "     |      Alias for field number 3\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Heapitem:\n",
      "     |  \n",
      "     |  __match_args__ = ('count', 'index', 'left', 'right')\n",
      "     |  \n",
      "     |  _field_defaults = {}\n",
      "     |  \n",
      "     |  _fields = ('count', 'index', 'left', 'right')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.tuple:\n",
      "     |  \n",
      "     |  __add__(self, value, /)\n",
      "     |      Return self+value.\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      Return key in self.\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __hash__(self, /)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __mul__(self, value, /)\n",
      "     |      Return self*value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __rmul__(self, value, /)\n",
      "     |      Return value*self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.tuple:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "    \n",
      "    class LineSentence(builtins.object)\n",
      "     |  LineSentence(source, max_sentence_length=10000, limit=None)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, source, max_sentence_length=10000, limit=None)\n",
      "     |      Iterate over a file that contains sentences: one line = one sentence.\n",
      "     |      Words must be already preprocessed and separated by whitespace.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      source : string or a file-like object\n",
      "     |          Path to the file on disk, or an already-open file object (must support `seek(0)`).\n",
      "     |      limit : int or None\n",
      "     |          Clip the file to the first `limit` lines. Do no clipping if `limit is None` (the default).\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      .. sourcecode:: pycon\n",
      "     |      \n",
      "     |          >>> from gensim.test.utils import datapath\n",
      "     |          >>> sentences = LineSentence(datapath('lee_background.cor'))\n",
      "     |          >>> for sentence in sentences:\n",
      "     |          ...     pass\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Iterate through the lines in the source.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PathLineSentences(builtins.object)\n",
      "     |  PathLineSentences(source, max_sentence_length=10000, limit=None)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, source, max_sentence_length=10000, limit=None)\n",
      "     |      Like :class:`~gensim.models.word2vec.LineSentence`, but process all files in a directory\n",
      "     |      in alphabetical order by filename.\n",
      "     |      \n",
      "     |      The directory must only contain files that can be read by :class:`gensim.models.word2vec.LineSentence`:\n",
      "     |      .bz2, .gz, and text files. Any file not ending with .bz2 or .gz is assumed to be a text file.\n",
      "     |      \n",
      "     |      The format of files (either text, or compressed text files) in the path is one sentence = one line,\n",
      "     |      with words already preprocessed and separated by whitespace.\n",
      "     |      \n",
      "     |      Warnings\n",
      "     |      --------\n",
      "     |      Does **not recurse** into subdirectories.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      source : str\n",
      "     |          Path to the directory.\n",
      "     |      limit : int or None\n",
      "     |          Read only the first `limit` lines from each file. Read all if limit is None (the default).\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      iterate through the files\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Text8Corpus(builtins.object)\n",
      "     |  Text8Corpus(fname, max_sentence_length=10000)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fname, max_sentence_length=10000)\n",
      "     |      Iterate over sentences from the \"text8\" corpus, unzipped from https://mattmahoney.net/dc/text8.zip.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Word2Vec(gensim.utils.SaveLoad)\n",
      "     |  Word2Vec(sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Word2Vec\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)\n",
      "     |      Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
      "     |      \n",
      "     |      Once you're finished training a model (=no more updates, only querying)\n",
      "     |      store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in ``self.wv``\n",
      "     |      to reduce memory.\n",
      "     |      \n",
      "     |      The full model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
      "     |      :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
      "     |      \n",
      "     |      The trained word vectors can also be stored/loaded from a format compatible with the\n",
      "     |      original word2vec implementation via `self.wv.save_word2vec_format`\n",
      "     |      and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sentences : iterable of iterables, optional\n",
      "     |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      "     |          See also the `tutorial on data streaming in Python\n",
      "     |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      "     |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
      "     |          in some other way.\n",
      "     |      corpus_file : str, optional\n",
      "     |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "     |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      "     |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      "     |      vector_size : int, optional\n",
      "     |          Dimensionality of the word vectors.\n",
      "     |      window : int, optional\n",
      "     |          Maximum distance between the current and predicted word within a sentence.\n",
      "     |      min_count : int, optional\n",
      "     |          Ignores all words with total frequency lower than this.\n",
      "     |      workers : int, optional\n",
      "     |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      "     |      sg : {0, 1}, optional\n",
      "     |          Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
      "     |      hs : {0, 1}, optional\n",
      "     |          If 1, hierarchical softmax will be used for model training.\n",
      "     |          If 0, hierarchical softmax will not be used for model training.\n",
      "     |      negative : int, optional\n",
      "     |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      "     |          should be drawn (usually between 5-20).\n",
      "     |          If 0, negative sampling will not be used.\n",
      "     |      ns_exponent : float, optional\n",
      "     |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      "     |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      "     |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      "     |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      "     |          other values may perform better for recommendation applications.\n",
      "     |      cbow_mean : {0, 1}, optional\n",
      "     |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      "     |      alpha : float, optional\n",
      "     |          The initial learning rate.\n",
      "     |      min_alpha : float, optional\n",
      "     |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      "     |      seed : int, optional\n",
      "     |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      "     |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      "     |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      "     |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      "     |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      "     |      max_vocab_size : int, optional\n",
      "     |          Limits the RAM during vocabulary building; if there are more unique\n",
      "     |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      "     |          Set to `None` for no limit.\n",
      "     |      max_final_vocab : int, optional\n",
      "     |          Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
      "     |          min_count is more than the calculated min_count, the specified min_count will be used.\n",
      "     |          Set to `None` if not required.\n",
      "     |      sample : float, optional\n",
      "     |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      "     |          useful range is (0, 1e-5).\n",
      "     |      hashfxn : function, optional\n",
      "     |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      "     |      epochs : int, optional\n",
      "     |          Number of iterations (epochs) over the corpus. (Formerly: `iter`)\n",
      "     |      trim_rule : function, optional\n",
      "     |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "     |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "     |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "     |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "     |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "     |          The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
      "     |          model.\n",
      "     |      \n",
      "     |          The input parameters are of the following types:\n",
      "     |              * `word` (str) - the word we are examining\n",
      "     |              * `count` (int) - the word's frequency count in the corpus\n",
      "     |              * `min_count` (int) - the minimum count threshold.\n",
      "     |      sorted_vocab : {0, 1}, optional\n",
      "     |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
      "     |          See :meth:`~gensim.models.keyedvectors.KeyedVectors.sort_by_descending_frequency()`.\n",
      "     |      batch_words : int, optional\n",
      "     |          Target size (in words) for batches of examples passed to worker threads (and\n",
      "     |          thus cython routines).(Larger batches will be passed if individual\n",
      "     |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      "     |      compute_loss: bool, optional\n",
      "     |          If True, computes and stores loss value which can be retrieved using\n",
      "     |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      "     |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      "     |          Sequence of callbacks to be executed at specific stages during training.\n",
      "     |      shrink_windows : bool, optional\n",
      "     |          New in 4.1. Experimental.\n",
      "     |          If True, the effective window size is uniformly sampled from  [1, `window`]\n",
      "     |          for each target word during training, to match the original word2vec algorithm's\n",
      "     |          approximate weighting of context words by distance. Otherwise, the effective\n",
      "     |          window size is always fixed to `window` words to either side.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
      "     |      \n",
      "     |      .. sourcecode:: pycon\n",
      "     |      \n",
      "     |          >>> from gensim.models import Word2Vec\n",
      "     |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      "     |          >>> model = Word2Vec(sentences, min_count=1)\n",
      "     |      \n",
      "     |      Attributes\n",
      "     |      ----------\n",
      "     |      wv : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      "     |          This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      "     |          directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Human readable representation of the model's state.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      str\n",
      "     |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
      "     |          and learning rate.\n",
      "     |  \n",
      "     |  add_null_word(self)\n",
      "     |  \n",
      "     |  build_vocab(self, corpus_iterable=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      "     |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      corpus_iterable : iterable of list of str\n",
      "     |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
      "     |      corpus_file : str, optional\n",
      "     |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "     |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      "     |          `corpus_file` arguments need to be passed (not both of them).\n",
      "     |      update : bool\n",
      "     |          If true, the new words in `sentences` will be added to model's vocab.\n",
      "     |      progress_per : int, optional\n",
      "     |          Indicates how many words to process before showing/updating the progress.\n",
      "     |      keep_raw_vocab : bool, optional\n",
      "     |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
      "     |      trim_rule : function, optional\n",
      "     |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "     |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "     |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "     |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "     |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "     |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      "     |          of the model.\n",
      "     |      \n",
      "     |          The input parameters are of the following types:\n",
      "     |              * `word` (str) - the word we are examining\n",
      "     |              * `count` (int) - the word's frequency count in the corpus\n",
      "     |              * `min_count` (int) - the minimum count threshold.\n",
      "     |      \n",
      "     |      **kwargs : object\n",
      "     |          Keyword arguments propagated to `self.prepare_vocab`.\n",
      "     |  \n",
      "     |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      "     |      Build vocabulary from a dictionary of word frequencies.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      word_freq : dict of (str, int)\n",
      "     |          A mapping from a word in the vocabulary to its frequency count.\n",
      "     |      keep_raw_vocab : bool, optional\n",
      "     |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      "     |      corpus_count : int, optional\n",
      "     |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      "     |      trim_rule : function, optional\n",
      "     |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "     |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "     |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "     |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "     |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "     |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      "     |          of the model.\n",
      "     |      \n",
      "     |          The input parameters are of the following types:\n",
      "     |              * `word` (str) - the word we are examining\n",
      "     |              * `count` (int) - the word's frequency count in the corpus\n",
      "     |              * `min_count` (int) - the minimum count threshold.\n",
      "     |      \n",
      "     |      update : bool, optional\n",
      "     |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      "     |  \n",
      "     |  create_binary_tree(self)\n",
      "     |      Create a `binary Huffman tree <https://en.wikipedia.org/wiki/Huffman_coding>`_ using stored vocabulary\n",
      "     |      word counts. Frequent words will have shorter binary codes.\n",
      "     |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
      "     |  \n",
      "     |  estimate_memory(self, vocab_size=None, report=None)\n",
      "     |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      vocab_size : int, optional\n",
      "     |          Number of unique tokens in the vocabulary\n",
      "     |      report : dict of (str, int), optional\n",
      "     |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dict of (str, int)\n",
      "     |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      "     |  \n",
      "     |  get_latest_training_loss(self)\n",
      "     |      Get current value of the training loss.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      float\n",
      "     |          Current training loss.\n",
      "     |  \n",
      "     |  init_sims(self, replace=False)\n",
      "     |      Precompute L2-normalized vectors. Obsoleted.\n",
      "     |      \n",
      "     |      If you need a single unit-normalized vector for some key, call\n",
      "     |      :meth:`~gensim.models.keyedvectors.KeyedVectors.get_vector` instead:\n",
      "     |      ``word2vec_model.wv.get_vector(key, norm=True)``.\n",
      "     |      \n",
      "     |      To refresh norms after you performed some atypical out-of-band vector tampering,\n",
      "     |      call `:meth:`~gensim.models.keyedvectors.KeyedVectors.fill_norms()` instead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      replace : bool\n",
      "     |          If True, forget the original trained vectors and only keep the normalized ones.\n",
      "     |          You lose information if you do this.\n",
      "     |  \n",
      "     |  init_weights(self)\n",
      "     |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n",
      "     |  \n",
      "     |  make_cum_table(self, domain=2147483647)\n",
      "     |      Create a cumulative-distribution table using stored vocabulary word counts for\n",
      "     |      drawing random words in the negative-sampling training routines.\n",
      "     |      \n",
      "     |      To draw a word index, choose a random integer up to the maximum value in the table (cum_table[-1]),\n",
      "     |      then finding that integer's sorted insertion point (as if by `bisect_left` or `ndarray.searchsorted()`).\n",
      "     |      That insertion point is the drawn index, coming up in proportion equal to the increment at that slot.\n",
      "     |  \n",
      "     |  predict_output_word(self, context_words_list, topn=10)\n",
      "     |      Get the probability distribution of the center word given context words.\n",
      "     |      \n",
      "     |      Note this performs a CBOW-style propagation, even in SG models,\n",
      "     |      and doesn't quite weight the surrounding words the same as in\n",
      "     |      training -- so it's just one crude way of using a trained model\n",
      "     |      as a predictor.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      context_words_list : list of (str and/or int)\n",
      "     |          List of context words, which may be words themselves (str)\n",
      "     |          or their index in `self.wv.vectors` (int).\n",
      "     |      topn : int, optional\n",
      "     |          Return `topn` words and their probabilities.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list of (str, float)\n",
      "     |          `topn` length list of tuples of (word, probability).\n",
      "     |  \n",
      "     |  prepare_vocab(self, update=False, keep_raw_vocab=False, trim_rule=None, min_count=None, sample=None, dry_run=False)\n",
      "     |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
      "     |      and `sample` (controlling the downsampling of more-frequent words).\n",
      "     |      \n",
      "     |      Calling with `dry_run=True` will only simulate the provided settings and\n",
      "     |      report the size of the retained vocabulary, effective corpus length, and\n",
      "     |      estimated memory requirements. Results are both printed via logging and\n",
      "     |      returned as a dict.\n",
      "     |      \n",
      "     |      Delete the raw vocabulary after the scaling is done to free up RAM,\n",
      "     |      unless `keep_raw_vocab` is set.\n",
      "     |  \n",
      "     |  prepare_weights(self, update=False)\n",
      "     |      Build tables and model weights based on final vocabulary settings.\n",
      "     |  \n",
      "     |  reset_from(self, other_model)\n",
      "     |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
      "     |      \n",
      "     |      Structures copied are:\n",
      "     |          * Vocabulary\n",
      "     |          * Index to word mapping\n",
      "     |          * Cumulative frequency table (used for negative sampling)\n",
      "     |          * Cached corpus length\n",
      "     |      \n",
      "     |      Useful when testing multiple models on the same corpus in parallel. However, as the models\n",
      "     |      then share all vocabulary-related structures other than vectors, neither should then\n",
      "     |      expand their vocabulary (which could leave the other in an inconsistent, broken state).\n",
      "     |      And, any changes to any per-word 'vecattr' will affect both models.\n",
      "     |      \n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "     |          Another model to copy the internal structures from.\n",
      "     |  \n",
      "     |  save(self, *args, **kwargs)\n",
      "     |      Save the model.\n",
      "     |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
      "     |      online training and getting vectors for vocabulary words.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to the file.\n",
      "     |  \n",
      "     |  scan_vocab(self, corpus_iterable=None, corpus_file=None, progress_per=10000, workers=None, trim_rule=None)\n",
      "     |  \n",
      "     |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      "     |      Score the log probability for a sequence of sentences.\n",
      "     |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      "     |      \n",
      "     |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      "     |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      "     |      \n",
      "     |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      "     |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      "     |      \n",
      "     |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      "     |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      "     |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      "     |      how to use such scores in document classification.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sentences : iterable of list of str\n",
      "     |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      "     |      total_sentences : int, optional\n",
      "     |          Count of sentences.\n",
      "     |      chunksize : int, optional\n",
      "     |          Chunksize of jobs\n",
      "     |      queue_factor : int, optional\n",
      "     |          Multiplier for size of queue (number of workers * queue_factor).\n",
      "     |      report_delay : float, optional\n",
      "     |          Seconds to wait before reporting progress.\n",
      "     |  \n",
      "     |  seeded_vector(self, seed_string, vector_size)\n",
      "     |  \n",
      "     |  train(self, corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=(), **kwargs)\n",
      "     |      Update the model's neural weights from a sequence of sentences.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      "     |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      "     |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      "     |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      "     |      you can simply use `total_examples=self.corpus_count`.\n",
      "     |      \n",
      "     |      Warnings\n",
      "     |      --------\n",
      "     |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      "     |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      "     |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.epochs`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      corpus_iterable : iterable of list of str\n",
      "     |          The ``corpus_iterable`` can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network, to limit RAM usage.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      "     |          See also the `tutorial on data streaming in Python\n",
      "     |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      "     |      corpus_file : str, optional\n",
      "     |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "     |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      "     |          `corpus_file` arguments need to be passed (not both of them).\n",
      "     |      total_examples : int\n",
      "     |          Count of sentences.\n",
      "     |      total_words : int\n",
      "     |          Count of raw words in sentences.\n",
      "     |      epochs : int\n",
      "     |          Number of iterations (epochs) over the corpus.\n",
      "     |      start_alpha : float, optional\n",
      "     |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      "     |          for this one call to`train()`.\n",
      "     |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      "     |          (not recommended).\n",
      "     |      end_alpha : float, optional\n",
      "     |          Final learning rate. Drops linearly from `start_alpha`.\n",
      "     |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      "     |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      "     |          (not recommended).\n",
      "     |      word_count : int, optional\n",
      "     |          Count of words already trained. Set this to 0 for the usual\n",
      "     |          case of training on all words in sentences.\n",
      "     |      queue_factor : int, optional\n",
      "     |          Multiplier for size of queue (number of workers * queue_factor).\n",
      "     |      report_delay : float, optional\n",
      "     |          Seconds to wait before reporting progress.\n",
      "     |      compute_loss: bool, optional\n",
      "     |          If True, computes and stores loss value which can be retrieved using\n",
      "     |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      "     |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      "     |          Sequence of callbacks to be executed at specific stages during training.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      .. sourcecode:: pycon\n",
      "     |      \n",
      "     |          >>> from gensim.models import Word2Vec\n",
      "     |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      "     |          >>>\n",
      "     |          >>> model = Word2Vec(min_count=1)\n",
      "     |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      "     |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors\n",
      "     |          (1, 30)\n",
      "     |  \n",
      "     |  update_weights(self)\n",
      "     |      Copy all the existing weights, and reset the weights for the newly added vocabulary.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  load(*args, rethrow=False, **kwargs) from builtins.type\n",
      "     |      Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.models.word2vec.Word2Vec.save`\n",
      "     |          Save model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to the saved file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`~gensim.models.word2vec.Word2Vec`\n",
      "     |          Loaded model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      "     |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      "     |      optionally log the event at `log_level`.\n",
      "     |      \n",
      "     |      Events are important moments during the object's life, such as \"model created\",\n",
      "     |      \"model saved\", \"model loaded\", etc.\n",
      "     |      \n",
      "     |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      "     |      but is useful during debugging and support.\n",
      "     |      \n",
      "     |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      "     |      will not record events into `self.lifecycle_events` then.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      event_name : str\n",
      "     |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      "     |      event : dict\n",
      "     |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      "     |          Can be empty.\n",
      "     |      \n",
      "     |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      "     |      \n",
      "     |          - `datetime`: the current date & time\n",
      "     |          - `gensim`: the current Gensim version\n",
      "     |          - `python`: the current Python version\n",
      "     |          - `platform`: the current platform\n",
      "     |          - `event`: the name of this event\n",
      "     |      log_level : int\n",
      "     |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Word2VecTrainables(gensim.utils.SaveLoad)\n",
      "     |  Obsolete class retained for now as load-compatibility state capture.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Word2VecTrainables\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      "     |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      "     |      optionally log the event at `log_level`.\n",
      "     |      \n",
      "     |      Events are important moments during the object's life, such as \"model created\",\n",
      "     |      \"model saved\", \"model loaded\", etc.\n",
      "     |      \n",
      "     |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      "     |      but is useful during debugging and support.\n",
      "     |      \n",
      "     |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      "     |      will not record events into `self.lifecycle_events` then.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      event_name : str\n",
      "     |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      "     |      event : dict\n",
      "     |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      "     |          Can be empty.\n",
      "     |      \n",
      "     |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      "     |      \n",
      "     |          - `datetime`: the current date & time\n",
      "     |          - `gensim`: the current Gensim version\n",
      "     |          - `python`: the current Python version\n",
      "     |          - `platform`: the current platform\n",
      "     |          - `event`: the name of this event\n",
      "     |      log_level : int\n",
      "     |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=4)\n",
      "     |      Save the object to a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname_or_handle : str or file-like\n",
      "     |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      "     |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      "     |      separately : list of str or None, optional\n",
      "     |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      "     |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      "     |          loading and sharing the large arrays in RAM between multiple processes.\n",
      "     |      \n",
      "     |          If list of str: store these attributes into separate files. The automated size check\n",
      "     |          is not performed in this case.\n",
      "     |      sep_limit : int, optional\n",
      "     |          Don't store arrays smaller than this separately. In bytes.\n",
      "     |      ignore : frozenset of str, optional\n",
      "     |          Attributes that shouldn't be stored at all.\n",
      "     |      pickle_protocol : int, optional\n",
      "     |          Protocol number for pickle.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.load`\n",
      "     |          Load object from file.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  load(fname, mmap=None) from builtins.type\n",
      "     |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to file that contains needed object.\n",
      "     |      mmap : str, optional\n",
      "     |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      "     |          via mmap (shared memory) using `mmap='r'.\n",
      "     |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |          Save object to file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      object\n",
      "     |          Object loaded from `fname`.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          When called on an object instance instead of class (this is a class method).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Word2VecVocab(gensim.utils.SaveLoad)\n",
      "     |  Obsolete class retained for now as load-compatibility state capture.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Word2VecVocab\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      "     |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      "     |      optionally log the event at `log_level`.\n",
      "     |      \n",
      "     |      Events are important moments during the object's life, such as \"model created\",\n",
      "     |      \"model saved\", \"model loaded\", etc.\n",
      "     |      \n",
      "     |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      "     |      but is useful during debugging and support.\n",
      "     |      \n",
      "     |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      "     |      will not record events into `self.lifecycle_events` then.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      event_name : str\n",
      "     |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      "     |      event : dict\n",
      "     |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      "     |          Can be empty.\n",
      "     |      \n",
      "     |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      "     |      \n",
      "     |          - `datetime`: the current date & time\n",
      "     |          - `gensim`: the current Gensim version\n",
      "     |          - `python`: the current Python version\n",
      "     |          - `platform`: the current platform\n",
      "     |          - `event`: the name of this event\n",
      "     |      log_level : int\n",
      "     |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=4)\n",
      "     |      Save the object to a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname_or_handle : str or file-like\n",
      "     |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      "     |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      "     |      separately : list of str or None, optional\n",
      "     |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      "     |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      "     |          loading and sharing the large arrays in RAM between multiple processes.\n",
      "     |      \n",
      "     |          If list of str: store these attributes into separate files. The automated size check\n",
      "     |          is not performed in this case.\n",
      "     |      sep_limit : int, optional\n",
      "     |          Don't store arrays smaller than this separately. In bytes.\n",
      "     |      ignore : frozenset of str, optional\n",
      "     |          Attributes that shouldn't be stored at all.\n",
      "     |      pickle_protocol : int, optional\n",
      "     |          Protocol number for pickle.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.load`\n",
      "     |          Load object from file.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  load(fname, mmap=None) from builtins.type\n",
      "     |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to file that contains needed object.\n",
      "     |      mmap : str, optional\n",
      "     |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      "     |          via mmap (shared memory) using `mmap='r'.\n",
      "     |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |          Save object to file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      object\n",
      "     |          Object loaded from `fname`.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          When called on an object instance instead of class (this is a class method).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    default_timer = perf_counter(...)\n",
      "        perf_counter() -> float\n",
      "        \n",
      "        Performance counter for benchmarking.\n",
      "    \n",
      "    score_sentence_cbow(...)\n",
      "        score_sentence_cbow(model, sentence, _work, _neu1)\n",
      "        Obtain likelihood score for a single sentence in a fitted CBOW representation.\n",
      "        \n",
      "            Notes\n",
      "            -----\n",
      "            This scoring function is only implemented for hierarchical softmax (`model.hs == 1`).\n",
      "            The model should have been trained using the skip-gram model (`model.cbow` == 1`).\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The trained model. It **MUST** have been trained using hierarchical softmax and the CBOW algorithm.\n",
      "            sentence : list of str\n",
      "                The words comprising the sentence to be scored.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            _neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            float\n",
      "                The probability assigned to this sentence by the Skip-Gram model.\n",
      "    \n",
      "    score_sentence_sg(...)\n",
      "        score_sentence_sg(model, sentence, _work)\n",
      "        Obtain likelihood score for a single sentence in a fitted skip-gram representation.\n",
      "        \n",
      "            Notes\n",
      "            -----\n",
      "            This scoring function is only implemented for hierarchical softmax (`model.hs == 1`).\n",
      "            The model should have been trained using the skip-gram model (`model.sg` == 1`).\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The trained model. It **MUST** have been trained using hierarchical softmax and the skip-gram algorithm.\n",
      "            sentence : list of str\n",
      "                The words comprising the sentence to be scored.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            float\n",
      "                The probability assigned to this sentence by the Skip-Gram model.\n",
      "    \n",
      "    train_batch_cbow(...)\n",
      "        train_batch_cbow(model, sentences, alpha, _work, _neu1, compute_loss)\n",
      "        Update CBOW model by training on a batch of sentences.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The Word2Vec model instance to train.\n",
      "            sentences : iterable of list of str\n",
      "                The corpus used to train the model.\n",
      "            alpha : float\n",
      "                The learning rate.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            _neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            compute_loss : bool\n",
      "                Whether or not the training loss should be computed in this batch.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the vocabulary actually used for training (They already existed in the vocabulary\n",
      "                and were not discarded by negative sampling).\n",
      "    \n",
      "    train_batch_sg(...)\n",
      "        train_batch_sg(model, sentences, alpha, _work, compute_loss)\n",
      "        Update skip-gram model by training on a batch of sentences.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2Vec.Word2Vec`\n",
      "                The Word2Vec model instance to train.\n",
      "            sentences : iterable of list of str\n",
      "                The corpus used to train the model.\n",
      "            alpha : float\n",
      "                The learning rate\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            compute_loss : bool\n",
      "                Whether or not the training loss should be computed in this batch.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the vocabulary actually used for training (They already existed in the vocabulary\n",
      "                and were not discarded by negative sampling).\n",
      "    \n",
      "    train_epoch_cbow(...)\n",
      "        train_epoch_cbow(model, corpus_file, offset, _cython_vocab, _cur_epoch, _expected_examples, _expected_words, _work, _neu1, compute_loss)\n",
      "        Train CBOW model for one epoch by training on an input stream. This function is used only in multistream mode.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The Word2Vec model instance to train.\n",
      "            corpus_file : str\n",
      "                Path to corpus file.\n",
      "            _cur_epoch : int\n",
      "                Current epoch number. Used for calculating and decaying learning rate.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            _neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            compute_loss : bool\n",
      "                Whether or not the training loss should be computed in this batch.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the vocabulary actually used for training (They already existed in the vocabulary\n",
      "                and were not discarded by negative sampling).\n",
      "    \n",
      "    train_epoch_sg(...)\n",
      "        train_epoch_sg(model, corpus_file, offset, _cython_vocab, _cur_epoch, _expected_examples, _expected_words, _work, _neu1, compute_loss)\n",
      "        Train Skipgram model for one epoch by training on an input stream. This function is used only in multistream mode.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The Word2Vec model instance to train.\n",
      "            corpus_file : str\n",
      "                Path to corpus file.\n",
      "            _cur_epoch : int\n",
      "                Current epoch number. Used for calculating and decaying learning rate.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            _neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            compute_loss : bool\n",
      "                Whether or not the training loss should be computed in this batch.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the vocabulary actually used for training (They already existed in the vocabulary\n",
      "                and were not discarded by negative sampling).\n",
      "\n",
      "DATA\n",
      "    CORPUSFILE_VERSION = 1\n",
      "    FAST_VERSION = 1\n",
      "    MAX_WORDS_IN_BATCH = 10000\n",
      "    logger = <Logger gensim.models.word2vec (INFO)>\n",
      "\n",
      "FILE\n",
      "    /Users/yzb/anaconda3/envs/model/lib/python3.11/site-packages/gensim/models/word2vec.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83wKQRBxoSPZ"
   },
   "source": [
    "# 查找最近的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-cNkE3foSPa",
    "outputId": "eae0396b-251e-43a1-98ea-cb8b75d37e14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('OA', 0.8478599190711975),\n",
       " ('商标注册', 0.831925094127655),\n",
       " ('泛微', 0.831880509853363),\n",
       " ('识别', 0.8070147633552551),\n",
       " ('试水', 0.8049625754356384),\n",
       " ('柜及', 0.8012972474098206),\n",
       " ('随心所欲', 0.7993997931480408),\n",
       " ('流媒体', 0.7984649538993835),\n",
       " ('叫车', 0.7980546951293945),\n",
       " ('安和力', 0.7939438819885254)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(['办公'],topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUFvr2lUoSPa"
   },
   "source": [
    "# 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "dmQb8m2RoSPa"
   },
   "outputs": [],
   "source": [
    "save_model_path='word2vec.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wafv8S_WoSPa",
    "outputId": "f9447c4d-b239-4c48-c8d9-24b2f922e7a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 09:54:23,092 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'word2vec.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-12-19T09:54:23.092532', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-arm64-arm-64bit', 'event': 'saving'}\n",
      "2023-12-19 09:54:23,094 : INFO : not storing attribute cum_table\n",
      "2023-12-19 09:54:23,124 : INFO : saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model.save(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpBkISEwoSPa"
   },
   "source": [
    "# 载入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4E-tCasCoSPb",
    "outputId": "ade19c7a-1c0e-402d-d188-c1878cbe31c3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-05 06:43:03,418 : INFO : loading Word2Vec object from word2vec.model\n",
      "2021-04-05 06:43:03,859 : INFO : loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "2021-04-05 06:43:03,862 : INFO : setting ignored attribute vectors_norm to None\n",
      "2021-04-05 06:43:03,866 : INFO : loading vocabulary recursively from word2vec.model.vocabulary.* with mmap=None\n",
      "2021-04-05 06:43:03,870 : INFO : loading trainables recursively from word2vec.model.trainables.* with mmap=None\n",
      "2021-04-05 06:43:03,876 : INFO : setting ignored attribute cum_table to None\n",
      "2021-04-05 06:43:03,877 : INFO : loaded word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec.load(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RNuA1D4_oSPb",
    "outputId": "fed496bb-93eb-44ed-ca0a-420dd3edaf35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('纯电', 0.8425928354263306),\n",
       " ('捷豹', 0.8233745098114014),\n",
       " ('上汽', 0.8183848857879639),\n",
       " ('智联', 0.8133119344711304),\n",
       " ('开瑞', 0.8040586709976196),\n",
       " ('X3', 0.7948211431503296),\n",
       " ('欧蓝德', 0.793942391872406),\n",
       " ('瑞虎', 0.792582631111145),\n",
       " ('T600', 0.7899853587150574),\n",
       " ('昂科威', 0.7895016670227051)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(['奇瑞'],topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TgYynPFoSPb"
   },
   "source": [
    "# 参考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gl6dGSp3oSPb"
   },
   "source": [
    "1. https://radimrehurek.com/gensim/models/word2vec.html "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "word2vec.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
